</div>
    <div id="intro">


    <div id="chapter2">
      <h3>2. Amazon Simple Storage Service (Amazon S3) and Amazon Glacier Storage</h3>
      <ul>
        <li>Common Amazon S3 use cases include backup and archive, web content, big data analytics, static website hosting, mobile and cloud native application hosting and disaster recovery.</li>
        <li>Amazon S3 integrated with many other AWS cloud services, including AWS IAM, AWS KMS, Amazon EC2, Amazon EBS, Amazon EMR, Amazon DynamoDB, Amazon Redshift, Amazon SQS, Amazon Lambda, and Amazon Cloud Front.</li>
        <li>Object storage differs from traditional block and file storage. Block storage manages data at a device level addressible blocks, while file storage manages data at the operating system level files and folders. <br>
          Object storage manages data as objects that contain both data and metadata, manupulated by an API.
        </li>
        <li>Amazon S3 buckets are containers for objects stored in Amazon S3, Bucket names must be globally unique. Each bucket is created in a specific region, and data doen't leave the region unless explicitly copied by the user.</li>
<li>Amazon S3 objects are files stored in buckets. Objects can be up to 5TB and can contain any kind of data. Objects contain both data and metadata and are identified by keys. Each amazon S3 bucket is can be addressed by unique URL formed by the webservices endpoint, the bucket name, and object key.</li>
        <li>Amazon S3 has minimalistic API - create/delete a bucket, read/write/delete objects, list keys in bucket - and uses a REST interface based on standard HTTP verbs - GET,PUT,POST and DELETE. <br>
        You can also use SDK wrapper libraries, the AWS CLI, and the AWS Management Console to work with Amazon S3.
        </li>
        <li>Amazon S3 is highly durable and highly available, designed for 11 nines of durability of objects in a given year and four nines of availability.</li>
        <li>Amazon S3 is eventually consistent, but offers read-after-write for new object PUTs. </li>
        <li>Amazon S3 objects are private by default, accessible only to the owners. Objects can be marked public readable to make them accessible on the web. Controlled access may be provided to others using ACLs and AWS IAM and Amazon S3 bucket policies. </li>
        <li>Static websites can be hosted in an Amazon S3 bucket.</li>
        <li>Prefixes and delimiters may be used in key names to organize and navigate data hierachically much like a traditional file system.</li>
        <li>Amazon S3 offers serveral storage classes suited for different usecases.<br>
         Standard is desinged for general purpose and needing high performance and low latency <br>
         Standard IA  is for less frequently accessed data. <br>
         RRS offfers lower redundancy at lower cost for easily reproduced data <br>
         Amazon Glacier offers low-cost durable storage for archive and long-term backups that can are rearly accessed and can accept a three to five hour retrieval time.
        </li>
        <li>Object life cycle management policies can be used to automatically move data between storage classes based on time.</li>
        <li>Amazon S3 data can be encrypted using server-side or client-side encryption, and encryption keys can be managed with Amazon KMS.</li>
        <li>Versioning and MFA delete can be used to protect against accidental deletion.</li>
        <li>Cross-region replication can be used to automatically copy new objects from a source bucket in one region to target bucket in another region.</li>
        <li>Pre-signed URLs grant time-limited permission to download objects and can be used to protect media and other webcontent from unauthorized "web scraping".</li>
        <li>Multipart upload can be used to upload large objects, and Range GET can be used to download options of an amazon S3 object or Amazon Glacier archive.</li>
        <li>Server access logs can be enabled on a bucket to track requestor, object, action, and response.</li>
        <li>Amazon S3 event notifications can be used to send an Amazon SQS or Amazon SNS message or to trigger an AWS Lambda function where an object is created or deleted.</li>
        <li>Amazon Glacier can be used as a standalone serivce or as a storage class in Amazon S3.</li>
        <li>Amazon Glacier stores data in achives, which are contained in vaults. You can have up to 1,000 vaults, and each vault can store an unlimted number of archives.</li>
        <li>Amazon Glacier valuts can be locked for compliance purposes.</li>
      </ul>

    </div>

    <div id="chapter3">
      <h3>3. Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Elastic Block Store (Amazon EBS)</h3>
      <ul>
        <li>Compute is the amount of computational power required to fullfill your workload. Amazon EC2 is the primary service to provide compute to customers.</li>
        <li>The instance type defines the virtual hardware supporting the instance. Available instance types vary in vCPUs, memory, storage, and network performance to address nearly any workload.</li>
        <li>An AMI defines the initial software state of the instance, both OS and applications. There are four sources of emphasis <br>
          AWS published generic OSs <br>
          Partner published AMIs in the AWS market place with software packages preinstalled. <br>
          Custom generated AMIs from existing Amazon EC2 instances, and uploaded AMIs from vitual servers. <br>
        </li>
        <li>Instances can be addressed by public DNS name, public IP address, or elastic IP address. <br>
          To access a newly launched Linux instance, use private half of the key pair to contact to the instance via SSH. <br>
          To access a newly created Windows instance, use private half of the key pair to decrypt the randomly initialized local admistrator password <br>
        </li>
        <li>Network traffic in and out of an instance can be controlled by virtual firewall called security group.
          A secuirty group allows rules that block traffic based on direction, port, protocol, and source/destination address.
        </li>
        <li>Bootstrapping allows you to run a script to initialize your instance with OS configurations and applications. This feature allows instance to configure themselves upon launch. <br>
          Once an instance is launched, you can change your instance type or, of Amazon VPC instances, the security group with which it is associated. <br>
        </li>

        <li>The three pricing options for instances are On-Demand, Reserved Instance, and Spot. <br>
          On-Demand has highest  per hour cost, requiring no up-front commitment and giving you complete control over the lifetime of the instance. <br>
          Reserved Instances require a commitment and provided a reduced overall cost over the lifetime of the reservation. <br>
          Spot instances are idle compute capacity that AWS makes available based on bid prices from customers. <br>
          The savings on per hour cost can be significant, but instances can be shutdown when bid price exceeds customer's current bid.
        </li>

        <li>Instance stores are block storage included with hourly cost of the instance. The amount and type of storage available varies with the instance type. <br>
          Instance store terminate when the associated instance is stopped, so they should only be used for temporary data or in architectures providing redundancy such as Hadoop's HDFS.
        </li>
        <li>
          Amazon EBS provides durable block storage in several types. <br>
          Magnetic has low cost per gigabyte and delivers modest performance. <br>
          General purpose SSD is cost-effective storage that can be provided up to 10,000 IOPS. <br>
          Provisioned IOPS SSD has the highest cost per gigabyte and is well suited for I/O-intensive workloads sensitive to storage performace. <br>
          Snapshots are incremental backups of Amazon EBS volumes store in Amazon S3. Amazon EBS volumes can be encrypted.
        </li>
      </ul>
    </div>

    <div id="chapter4">
      <h3>4. Amazon Virtual Privite Cloud (Amazon VPC)</h3>
      <ul>
        <li>Amazon VPC allows you to create your own private virtual network within the cloud. <br>
          You can provision your own logically isolated section of AWS similar to designing and implementing a seperate independent network that you'd operate in a physical data center.
        </li>
        <li>A VPC consists of following components: <br>
        - Subnet <br>
        - Route tables <br>
        - DHCP Option sets <br>
        - Security Groups <br>
        - Network ACLs
        </li>
        <li>A VPC has the following optional components: <br>
        - IGWs <br>
        - EIP addresses <br>
        - Endpoints <br>
        - Peering <br>
        - NAT instance and NAT gateway <br>
        - VPG, CGW, and VPN
        </li>
        <li>Subnets can be public, private, or VPN-only.<br>
        - A public subnet is one in which the assouciated route table directs the subnet's traffic to the Amazon VPC's IGW. <br>
        - A private subnet is one in which the associated route table doesn't directs the subnet's traffic to the Amazon VPC's IGW. <br>
        - A VPN-only subnet is one in which the associated route table directs the subnet's traffic to the Amazon VPC's VPG and does not have route to IGW.<br>
        Regarless of the type of subnet, the internal IP address range of the subnet is alway's private (non-routable on the Internet).
        </li>
        <li>
        A route table is logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where the nework traffic is directed. <br>
        A route table's route are what permits Amazon EC2 instances within different subnets within an Amazon VPC to communicate with each other. <br>
        You can modify routetable and add yor own custom routes. You can also use route tables to specify which subnets are public (by directing internet traffic to the IGW) and which subnets are private (by not having route a route that directs traffic to the IGW)<br>
        An IGW is a horizantally scaled, redundant, and highly available Amazon VPC component that allows communicaion between instances in your Amazon VPC and the Internet. <br>
        IGWs are fully redundant and have no bandwidth constraints. An IGW provides a target in your Amazon VPC route table for Internet-routable traffic and it performs network address translation for instances that have been assigned public IP addesses.
        </li>
        <li>
        The DBCP option sets element of Amazon VPC allows you to direct Amazon EC2 host name assignment to your own resources. <br>
        Inorder for you to assign you own domain name to your instances, you create a custom DHCP option set and assign it to your Amazon VPC.
       </li>
        <li>
        An EIP address is a static, public IP address in the pool for the region that you can allocate to your account (pull from the pool) and release (return to the pool). <br>
        EIPs allow you to maintain a set of IP addresses that remain fixed while the underlying infrastructure may change orvertime.
        </li>
        <li>
        An Amazon VPC endpoint enables you to create a private connection between your Amazon VPC and another AWS service without requiring access over the Internet or through NAT instace, VPN connction, or AWS Direct Connect. <br>
        You can create multiple end points for single service, and you can use different route tables to enforce different access policies from different subnets to the same service.
        </li>
        <li>
        An Amazon VPC peering connection is a network connection between two Amazon VPCs that enables instances in either Amazon VPC to communicate with each other as if they were within the same network <br>
        You can create an Amazon VPC peering connection between your own Amazon VPCs or with an Amazon VPC in another AWS account within single region. <br>
        A peering connection is neither a gateway not a VPN connection and does not introduce a single point of failure for communication.
        </li>
        <li>
        A Security Group is a virtual stateful firewall that controls inbound and outbound traffic to Amazon EC2 instances. <br>
        When you first launch an Amazon EC2 instance in to Amazon VPC, You must specify a security group with which it will be associated. <br>
        AWS provides a defaut security group for you to use, which has rules that allow all instances associated with the security group to communicate with each other and allow all outbound traffic. <br>
        You may change the rules for default security group, but you may not delete the default security group. <br>
        </li>
        <li>A network ACL is another layer of security that acts as stateless firewall on a subnet level. <br>
        Amazon VPCs are created with a modifiable default network ACL associated with every subnet that allows all inbound and outbound traffic. <br>
        If you want to create custom network ACL, its initial configuration will deny all inbound and outbound traffic until you create a rule that states otherwise.
        </li>
        <li>A NAT instance is customer-mmanaged instance that is designed to accept traffic from instances within private subnet, translate the source IP address to the public IP address of NAT instance, and forward the traffic to the IGW. <br>
        In addition, The NAT instance maintains the state of the forwarded traffic in order to retrun response traffic from the Intenet to the proper in the private subnet.
        </li>
        <li>A NAT gateway is an AWS-managed service that is designed to accept traffic from instances within private subnet, translate the source IP address to the public IP address of NAT gateway, and forward the traffic to the IGW. <br>
        In addition, The NAT instance maintains the state of the forwarded traffic in order to retrun response traffic from the Intenet to the proper in the private subnet.
        </li>
        <li>
        A VPG is th VPN concentrator on the AWS side of the VPN connection between the two networks. <br>
        A CGW is a physical device or software application on the customer's side of the VPN connection. <br>
        After these two elements of Amazon VPC is created, the last step is to create VPN tunnel. The VPN is established after traffic is generated from the customer's side of VPN connection.
        </li>
      </ul>
    </div>

    <div id="chapter5">
      <h3>5. Elastic Load Balancing, Amazon CloudWatch, and Auto Scaling</h3>
      <ul>
        <li>Elasitic Load Balancing, which is used to distribute traffic across a group of Amazon EC2 instances in one or more Availability Zones to achieve greater level of fault tolerance for your applications.</li>
        <li>
          - Amazon CloudWatch, which monitors resources and applications.<br>
          - Amazon CloudWatch is used to collect and track metrics, create alams that send notifications, and make changes to resources being monitored based on rules you define.
        </li>
        <li>
          Auto Scaling, which allows you to automatically scale your Amazon EC2 capacity out and in using criteria that you define.
        </li>
        <li>
          These three services can be used very effectively together to create a highly available application with a resilient architecture on AWS.
        </li>
      </ul>
    </div>

    <div id="chapter6">
      <h3>6. AWS Identity and Access Management (IAM)</h3>
      <ul>
        <li>
          - IAM is a powerful service that gives you ability to control which people and applications can access your AWS account at a very granular level. <br>
          - Because a root user in an AWS account cannot be limited, you should setup IAM users and temporary security tokens for your people and processes to interact with AWS.
        </li>
        <li>
          - Policies define what actions can and cannot be taken. <br>
          - Policies are associated with IAM users either directly or group membership. <br>
          - A temporary security token is associated with a policy by assuming an IAM role. <br>
          - You can use your own policies or use one of the managed policies provided by AWS.
        </li>
        <li>
          - Common usecases of IAM roles include federating identies from external IdPs, assigning privileges to an Amanzon EC2 instance where they can be assumed by applications running on the instance, and cross-account access.
        </li>
        <li>
          - IAM user accounts can be further secured by rotating keys, implementing MFA, and adding conditions to policies. <br>
          - MFA ensures that authentication is based on something thing you have inaddition to something you know, and conditions can add further restrications such as limiting client IP address ranges or setting a particular time interval.
        </li>
      </ul>
    </div>


    <div id="chapter7">
      <h3>7. Databases and AWS</h3>
      <ul>
        <li>Amazon RDC manages the heavy lifting involved in adminstering a database infrastructure and software and lets you focus on building the relational schemas that best fit your usecase and the performance tuning to optimize your queries. </li>
        <li>
          - Amazon RDC supports popular open-source and commercial database engines and provides a consistent operational model for common administrative tasks. <br>
          - Increase your availability by running master-slave configuration across Availability Zones using Multi-AZ deployments. <br>
          - Scale your application and increase your database read performance using read replicas.
        </li>
        <li>
          - Amazon Redshift allows you to deploy a datawarehouse cluster that is optimized for analytics and reporting workloads within minutes. <br>
          - Amazon Redshift distributes your records using columnar storage and parallelizes your query execution across multiple computer nodes to deliver fast query performance. <br>
          - Amazon Redshift clusters can be scaled up or down to support large, petabyte-scale databases using SSD and magnetic disk storage.
        </li>
        <li>
          - Connect to Amazon Redshift clusters using standard SQL clients with JDBC/ODBC drivers and execute SQL queries using many of the same analytics and ETL tools that you use today.<br>
          - Load data into your Amanzon Redshift clusters using COPY command to bulk import flat files stored in Amazon S3, then run standard SELECT command to search and query the tables.
        </li>
        <li>
          - Back up both your Amazon RDS and Amazon Redshift clusters using automated and manual snapshots to allow for point-in-time recovery. <br>
          - Secure your Amazon RDS and Amanzon Redshift datbases using a combination of IAM, dabase-level access control, networklevel access control, and data encryption techniques. <br>
        </li>
        <li>
          Amazon DynamoDB simplifies the administration and operations of NoSQL database in the cloud <br>
        </li>
      </ul>
    </div>

    <div id="chapter8">
      <h3>8. SQS, SWF, and SNS</h3>
      <ul>
        <li>
          - Amazon SQS is a unique service designed by Amazon to help you decouple your infrastructure. <br>
          - Using Amazon SQS, you can store messages on reliable and scalable infrastructure as they travel between distributed components of your application that perform different tasks, without losing messages or requiring each component to contineously available.
        </li>
        <li>
          - Understand Amazon SQS queue operations, unique IDs, and metadata. <br>
          - Be familiar with queue and message identifiers such as queue URLs an message Ids, and reciept handles. <br>
          - Understand related concepts such as delay queues, message attributes, long polling, message timers, dead letter queues, access control, and the overall message lifecycle.
        </li>
        <li>
          - Amazon SWF allows you to create applications that coordinte work across distributed components. <hr>
          - Amazon SWF is driven by tasks, which are logic unit of work that different components of your application peform. <br>
          - To manange tasks across your application, you need to be aware of inter-task dependencies, scheduling of tasks, and using tasks concurrenlty. <br>
          - Amazon SWF simplifies the coordination of workflow tasks, giving you full control over their implementation without worrying about underlying complexities such as tracking their progress and maintaining their state.
        </li>
        <li>
          You must be familiar with following Amazon SWF components and the lifecycle of a workflow execution: <br>
          - Workers, starters, and deciders <br>
          - Workflows <br>
          - Workflow bistory <br>
          - Actors <br>
          - Tasks <br>
          - Domains <br>
          - Object identifiers <br>
          - Task lists <br>
          - Workflow execution closure <br>
          - Long polling
        </li>
        <li>
          - Amazon SNS is a push notification service that lets you send individual or multiple messages to large number of recepients. <br>
          - Amazon SNS consists of two types of clients : Publishers and Subscribers (Sometimes known as producers and consumers). <br>
          - Publishers communicate to subscribers asynchronously by sending message to a topic. <br>
          - A topic is simply a logical access point/communication channel that contains a list of subscribers and the methods used to communicate to them.<br>
          - When you send a message to a topic, it is automatically forwarded to each subscriber of that topic using the communication method configured for that subscriber.
        </li>
        <li>
          Amazon SNS can support a wide variety of needs, including monitoring applications, workflow systems, time-sensitive information updates, mobile applications, and any other application that genereates or consume notifications.<br>
          Understand some common Amazon SNS scenairos, including: <br>
          - Fanout <br>
          - Application and System alerts <br>
          - Push email and text messaging <br>
          - Mobile push notifications
        </li>
      </ul>
    </div>

    <div id="chapter9">
      <h3>9. Domain Name System (DNS) and Amazon Route 53</h3>
      <ul>
        <li>
          - DNS, Which is the methodology that computers use to convert humun-friendly domain names (for example, amazon.com) into IP addresses (such as 192.0.0.1). <br>
          - DNS starts with TLDs (for example .com, .edu). The Internet Assigned Numbers Authority (IANA) controls the TLDs in a root zone database, which is essentially a database for all available TLDs. <br>
          - DNS names are registered with a domain registrar. A registrar is an authority that can assign domain names directly under one or more TLDs. <br>
          - These domains are registered with IntenetINC, a service of ICANN, which enforces the uniquenss of domain names acorss the Internet. <br>
          - Each domain name become registered in a central database, known as the WhoIS database.
        </li>
        <li>
          DNS consists of a number of different record types, including but not limited to the following:
          - A
          - AAAA
          - CNAME
          - MX
          - NS
          - PTR
          - SOA
          - SPF
          - TXT
        </li>
        <li>
          - Amazon Route 53 is a highly available and higly scalable AWS-provided DNS service. <br>
          - Amazon Route 53 connects user requests to infrastructure running on AWS (for example, Amazon EC2 instances and Elastic Load Balancing load balancers). <br>
          - It can also be used to route users to infrastructure outside of AWS.
          - With Amazon Route 53, your DNS records are organized into hosted zones that you configure with Amazon Route 53 API. <br>
          - A hosted zone simply stores records for your domain. These records consists of A, CNAME, MX, and other supported record types.
        </li>
        <li>
          Amazon Route 53 allows you to have several different routing policies, including the following: <br>
          - Simple : Most commonly used when you have a single resource that performs a given function for your domain. <br>
          - Weighted : Used when you want to route a percentage of your traffic to one particular resource or resources. <br>
          - Latency Based : Used to route your traffic based on the lowest latency so that your user get the fastest response time. <br>
          - Failover : Used for DR and to route your traffic from your resources in a primary location to standby location.
          - Geolocation : Used to route your traffic based on your end user's location.
        </li>
        <li>
          - Remember to pull these concepts together to build an application that is highly available and resilient to failures. <br>
          - Use Elasitic Load Balancing across Availability Zones with connection draining enabled, use health checks defined to ensure that the application delegates requests only to healthly Amazon EC2 instances, and use a latency-based routing policy with Elastic Load Balancing health checks to ensure requests are routed with minimal latency to clients. <br>
          - Use Amazon CloudFront edge locations to spread content all over the world with minimal client latency. <br>
          - Deploy the application in multiple AWS regions, protecting it from regional outage.
        </li>
      </ul>
    </div>

    <div id="chapter10">
      <h3>10. Amazon ElasticCache</h3>
        <ul>
          <li>
            - You can quickly launch clusters running Memcached or Redis to store frequetly used data in-memory. <br>
            - Caching can speed up the reponse time of your application, reduce load on your back-end data stores, and improve user experience.<br>
            - With Amazon ElasticCache, you can offload the administrative tasks for provisioning and operating clusters and focus on the application.<br>
            - Each cache cluster contains one or <i>nodes</i>. Select from a range of <i>node types</i> to give right mixture of compute and memory resources for your use case. <br>
          </li>
          <li>
            - You can expand both Memcached and Redis clusters vertically by selecting a large or smaller node type to match your needs. <br>
            - With Amazon ElasiticCache and Memcached engine, you can also scale your cluster horizantally by adding or removing nodes. <br>
            - With Amazon ElastticCache and Redis engine, you can also scale horizantally by creating <i>replication group</i> that will automatically replicate across multiple read replicas.
          </li>
          <li>
            - Streamline your backup and recovery process for Redis clusters with Amazon ElasticCache's consistent operational model. <br>
            - While Memcached clusters are in-memory only and cannot be persisted, Redis clusters supports both automated and manual <i>snapshots</i>.
            - A snapshot can then be restored to recover from a failure or to clone an environment.
          </li>
          <li>
            - You can secure your cache envronments at network level with security group and network ACLs, and at the infrastructure level using IAM policies. <br>
            - Security groups will serve as your primary access control mechanism to restrict inbound access for active clusters. <br>
          </li>
          <li>
            - You should analyze your data usage patterns and identify frequently run queries or other exensive operations that could be candidate for caching. <br>
            - You can relieve pressure from your database by offloading read requests to cache tier. <br>
            - Data elements that are accessed on evey page load, or with every request but not change, are often prime candidates for caching. <br>
            - Even data that changes frequently can often benefit from being cached with very large request volumes.
          </li>
        </ul>
    </div>


    <div id="chapter11">
      <h3>11. Additional Key Serivces</h3>
      <ul>
        <li>
         These services are grouped in to four categories of services <br>
         - Storage and Content Delivery <br>
         - Security <br>
         - Analytics <br>
         - DevOps
       </li>
       <li>
         - In the storage and content delivery group, we covered Amazon CloudFront and AWS Storage Gateway.<br>
         - Amazon CloudFront is a global CDN service. It integrates with other AWS products to give developers and business an easy way to distribute content to end uses with low latency, high transfer speeds,and no minimum usage commitments. <br>
         - AWS Storage Gateway is a service that connects an on-promise software appliance with cloud-based storage. <br>
         - It provides seamless and secure integration between organization's on-promises IT environment and AWS storage infrastructure. <br>
         - The AWS Storage Gateway appliance maintence frequenlty accessed data on-premises while encrypting and storing all of your data in Amazon S3 or Amazon Glacier.
       </li>
       <li>
         - The services we covered in security focused on Identity Management (AWS Directory Service), Key Management (AWS KMS, AWS CloudHSM) and Audit (AWS Cloud Trail). <br>
         - AWS Directory Service is managed service offering, providing directories that contain information about your organization, including users, groups, computers, and other resources. <br>
         - AWS Directory Service is offered in three types : AWS Directory Service for Microsoft Active Directory (Enterprise Edition), Simple AD, and AD connector.
       </li>
       <li>
         - Key management is the management of cryptographic keys within cryptosystem. This includes dealing with the generation, exchange, storage, use, and replacement of keys. <br>
         - AWS KMS is managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. <br>
         - AWS KMS lets you create keys that can never be exported from the service and that can be used to encrypt and decrypt data based on policy you define. <br>
         - AWS CloudHSM helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated HSM appliances with the AWS cloud. <br>
         - An HSM is a hardware appliance that provides secure key storage and cryptographic operations within a temper-resistant hardware module.
       </li>
       <li>
         - Rounding out security service is AWS CloudTrail. AWS CloudTrail provides visibility into user activity by recording API calls made on your account. <br>
         - AWS CloudTrail records important information about each API call, including name of the API, the identity of the caller, the time of the API call, the request parameters and the response elements returned by the AWS service. <br>
         - This information helps you to track changes made to your AWS resources and troubleshoot operational issues.
       </li>
       <li>
         - The analytics services covered to help you overcome the unique list of challenges associated with big data in today's IT world. <br>
         - Amazon Kenesis is a platform for handling massive streaming data on AWS, offers powerful services to make it easy to load and analyze the streaming data and also providing the ability for you to build custom streaming data applications for specialized needs. <br>
         - Amazon EMR provides you with a fully managed, on-demand Hadoop framework. <br>
         - The reduction of complexity and up-front cost combined with the scale of AWS means you can instantly spin up large Hadoop clusters and starts processing within minutes.
       </li>
       <li>
         - To supplement the big data challendges, orchestrating data movements comes with its own challenges. <br>
         - AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, and also on-promises data sources, at specified intervals. <br>
         - With AWS Data Pipeline, you can regularly access your data where it's stored, transform and process it at scale, and efficiently transfer the results to AWS services such as Amazon S3, Amazon RDS, Amazon DynamoDB and Amazon EMR. <br>
         - Additionally, AWS Import/Export is a service that accelarates transferring large amounts of date into an out of AWS using physical storage applications, bypassing the Internet. <br>
         - The data is copied to a device at the source, shipped via standard shipping mechanism, and then copied to the destination.
       </li>
       <li>
         - AWS continues to evolve services in support of organization embracing DevOps.
         - Services such as AWS OpsWorks, AWS CloudFormation, AWS Elastic Beanstalk, and AWS Config are leading the way for DevOps on AWS. <br>
         - AWS OpsWorks provides a configuration management service that helps you to configure and operate applications using Chef. <br>
         - AWS OpsWorks works with applications of any level of complexity and is independent of any particular architectural pattern. <br>
         - AWS CloudFormation allows organizations to deploy, modify, and update resources in a controlled and predictable way, in effect applying version control to AWS infrastructure the same way on would do with software. <br>
       </li>
       <li>
         - AWS Elastic Beanstalk allows developers to simply upload their application code, and the service automatically handles all of the details such as resource provisioning, load balancing, Auto Scalling, and monitoring. <br>
         - AWS Config delivers a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. <br>
         - With AWS Config, organizations have the information necessary for compliance auditing, security analysis, resource change tracking, and troubleshooting.
       </li>
      </ul>
    </div>


    <div id="chapter12">
      <h3>12. Security on AWS</h3>
      <ul>
        <li>
          - Security within AWS is based on a "defence in depth" model where no one, single element is used to secure systems on AWS. <br>
          - Rather, AWS uses multitude of elements - each acting at different layers of the system - in total to secure the system. <br>
          - AWS is responsible for some layers of this model, and customers are responsible for others. <br>
          - AWS also offers security tools and features of services for customers to use at their discretion.
        </li>
        <li>
          <h3>Security Model</h3>
          - The shared responsibility model is the security model where AWS is responsible for the security of underlying cloud infrastructure, and customer is responsible for securing workloads deployed in AWS. <br>
          - Customers benefit from a data centre and netwrork architecture built to satisfy the requirements of AWS most security-sensitive customers.<br>
          - This means that customers get a resilient infrastructure, designed for high security, without the capital outlay and operational overhead of a traditional data centre.
        </li>
        <li>
          <h3>Account Level Secuirty</h3>
          - AWS credentials help ensure that only authorized users and processes access your AWS account and resources. <br>
          - AWS uses sevaral types of credentials for authentication. These include passwords, cryptographic keys, digital signatures, and certificates. <br>
          - AWS also provides the option of requiring MFA to log in to your AWS account or IAM user accounts.
          - passwords are required to access your AWS account, individual IAM user account, AWS Discussion Forms and the AWS Support Centre. <br>
          - You specify the password when you first create account, and you can change it at any time by going to Security Credentials page.
        </li>
        <li>
          - AWS MFA is an addiional layer of security for accessing AWS Cloud services.
          - When you enable this optional feature, you will need to provide a six-digit, single-use code in addition to your standard user name and password credentials before access is granted to your AWS account settings or AWS CloudServices or resources, <br>
          - You get this single-use code from an authentication device that you keep in your physical posession. <br>
          - This is multi-factor because more than one authentication factor is checked before access is granted: a password (something you know) and the precise code from your authentication device (something you have).  <br>
          - An MFA device uses a software application that generates six-digit authentication codes that compatible with the TOTP standard, as described in RFC 6238.
        </li>
        <li>
          - Access keys are created by AWS IAM and delivered as pair: The Access Key ID (AKI) and Secret Access Key (SAK).
          - AWS requires that all API requests be signed by SAK; that is, they must include a digital signature  that AWS can use to verify the identity of the requester. <br>
          - You calculate the digital signature using a cryptographic has function. If you use any of AWS SDKs to generate requests, the digital signrature calculation is done for you. <br>
          - The most recent version of the digital signature calculation process at the time of this writing is Signature version 4, which calculates the signature using HMAC-SHA-256 protocol. <br>
          - AWS CloudTrail is a webservice that records API calls made on your account and delivers log files to your Amazon S3 bucket.
          - AWS CloudTrail's benefit is visibility into account activity by recording API calls made on your account.
        </li>
        <li>
          <h3>Service-Specific Security</h3>
          - In addition to the Shared Responsibility Model and Account Level Security, AWS offers security features for each of the services it provides.
        </li>
        <li>
          <h4>Compute - Amazon EC2: </h4>
          - Amazon EC2 supports RSA 2048 SSH-2 Key pairs for gaining first access to an Amazon EC2 instance. <br>
          - On Linux instance, access is granted through showing pocession of the SSH private key. <br>
          - On Windows instance, access is granted by shosing pocession of the SSH private key in order to decrypt the administrator password. <br>
        </li>
        <li>
          <h4>Compute - Amazon EBS: </h4>
          - Data stored in Amazon EBS is redundantly stored in multiple physical locations within the same Availability Zone as part of normal operation of that service and at no additional charge. <br>
          - AWS provides the ability to encrypt Amazon EBS volumes and their snapshots with AES-256. <br>
          - The encryption occurs on the servers that host Amazon EC2 instance, providing encryption of data as it moves between Amazon EC2 instance and Amazon EBS storage.
        </li>
        <li>
          <h4>Networking - Elastic Load Balancing</h4>
          - Elastic Load Balancing configures your load balancer with a pre-defined cipher set that is used for TLS negotiation when a connection established a client and your load balancer. <br>
          - The pre-defined cipher set provides compatibility with a broad range of clients and uses strong cryptographic algorithms. <br>
          - Elastic Load Balancing allows you to identify the originating IP address of a client connecting to your servers, whether you're using HTTPS or TCP load balancing.
        </li>
        <li>
          <h4>Networking - Amazon VPC</h4>
          - Amazon VPC enables you to create an isolated portion of the AWS cloud and launch Amazon EC2 instances that have private (RFC 1918) addresses in the range of your choice. <br>
          - Security features within Amazon VPC include security groups, network ACLs, routing tables, and external gateways. <br>
          - Each of these items is complementory to provide secure, isolated network that can be extended through selective enabling of direct Internet access or private connectivity to another network.
        </li>
        <li>
          <h4>Networking - Amazon CloudFront</h4>
          - Amazon CloudFront gives customers and easy way to distrubute content to end users with low latency and high data transfer speeds. <br>
          - It delivers static, dynamic and streaming content using global network of edge loactions. <br>
          - To control access to original copies of your objects in Amazon S3, Amazon CloudFront allows you to create one or more Origin Access Identifiers and associate these with your distributions. <br>
          - To control who can download objects from Amazon CloudFront edge locations, the service uses a signed-URL verification system.
        </li>
        <li>
          <h4>Storage - Amazon S3</h4>
          - Amazon S3 allows you to upload and retrieve data any time, from anywhere on the web. <br>
          - Access to data stored in Amazon S3 is restricted by default; only bucket and object owners have access to the Amazon S3 resources they create. <br>
          - You can securly upload and down the data to Amazon S3 via SSL-encrypted endpoints. <br>
          - Amazon S3 support several methods to encrypt data at rest.
        </li>
        <li>
          <h4>Storage - Amazon Glacier</h4>
          - Amazon Glacier service provides low cost, secure, and durable storage. <br>
          - You can securly upload and download data to Amazon Glacier using SSL-encrypted endpoints, and the service automatically encrypts the data using AES-256 and stores it durably in an immutable form.<br>
        </li>
        <li>
          <h4>Storage - Amazon Storage Gateway</h4>
          - AWS Storage Gateway Service connects you on-promises software appliance with cloud-based storage to provide seamless and secure integration between your IT environment and AWS storage infrastructure.<br>
          - Data is asynchronously transferred from your on-promises storage hardware to AWS over SSL and stored encrypted in Amazon S3 using AES-256.
        </li>
        <li>
          <h4>Database - Amazon DynamoDB</h4>
          - Amazon DynamoDB is managed NoSQL database service for provides fast and predictable peformance with steamless steability. <br>
          - You can control access at database level by creating database-level permissions that allow or deny access to items (rows) and attributes (columns) based on the needs of your application.
        </li>
        <li>
          <h4>Database - Amazon RDS</h4>
          - Amazon RDS allow you to quickly create relational database instance and flexibly scale the associated compute resources and storage capacity to meet the application demand. <br>
          - You can control Amazon RDS DB instance access via DB security groups, which acts as firewall controlling network access to your DB instance. <br>
          - Database secuty groups default to deny all access mode, and customers must specifically authorize network ingress. <br>
          - Amazon RDS is supported within an Amazon VPC, and for multi-AZ deployments, and defining subnet for all Availability Zones in a region will allow Amazon RDS to create a new standby in another Availability Zone should the need arise. <br>
          - You can encrypt connections between your application and your DB instance using SSL, and you can encrypt data at rest with in Amazon RDS instances for all database engines.
        </li>
        <li>
          <h4>Database - Amazon Redshift</h4>
          - Amazon Redshift is a petabyte-scale SQL data wearhouse service that runs on highly optimized and managed AWS compute and storage resources. <br>
          - The service enables you to create configure firewall rules (security groups) to control your network access to your database wearhouse cluster. <br>
          - Database users are named user accounts that can connect to a database and are authenticated when they log in to Amazon Redshift. <br>
          - In Amazon Redshift, you grant database user permissions on a per-cluster basis instead of per-table basis. <br>
          - You may choose Amazon Redshit to store all data in user-created tables in an encrypted format using hardware-accelarated AES-256 block encryption keys. This includes all data written to disks and also any backups. <br>
          - Amazon Redshift uses a four-tier, key-based architecture for encryption. These keys consist of data encryption keys, a database key, a cluster key, and a master key.
        </li>
        <li>
          <h4>Database - Amazon ElasticCache</h4>
          - Amazon ElasticCache is a web service that makes it easy to set up, manage, and scale distributed in-memory cache environments in the cloud. <br>
          - Amazon ElasticCache allows you to control access to your Cache Clusters using Cache Security Groups. <br>
          - A Cache Security Group acts like a firewall, controlling network access to your Cache Cluster.
        </li>
        <li>
          <h4>Database - Amazon SQS</h4>
          - Amazon SQS is highly reliable, scalable messaging queuing service that enables asynchronous message-based communication between distributed components. <br>
          - Amazon SQS access is granted based on an AWS account or a user created with AWS IAM. <br>
          - Data stored in SQS is not encrypted by AWS; however the user can encrypt data before it is uploaded to AmazonSQS, provided that application using the queue has means to decrypt the message when it's retrieved.
        </li>
        <li>
          <h4>Database - Amazon SNS</h4>
          - Amazon SNS is a web service that make it easy to setup, operate, and send notifications from the cloud. <br>
          - It provides developers with a highly scalable, flexible, and cost effective capability to publish messages from an application and immedietly deliver them to subscribers or other applications. <br>
          - Amazon SNS allows topic owners to setup policies for a topic that restrict who can publish or subscribe to a topic.
        </li>
        <li>
          <h4>Analytics - Amazon EMR</h4>
          - Amazon EMR is a managed webservice you use to run Hadoop clusters that process vast amounts of data by distributing the work and data among several servers. <br>
          - When launching job flows on your behalf, Amazon EMR sets up two Amazon EC2 secuirity groups : one for master nodes and another for slaves. <br>
          - You can launch Amazon EC2 instances of your Amazon EMR cluster in to Amazon VPC, which is like launching it in to private subnet. <br>
          - You can encrypt the input data before you upload it to Amazon S3 using any common data encryption tool. <br>
          - If you do encrypt the data befor it is uploadeed, you then need to add a decryption step to the beginning of your data flow when EMR feteches the data from Amazon S3.
        </li>

        <li>
          <h4>Analytics - Amazon Kinesis</h4>
          - Amazon Kinesis is a managed service designed to handle real-time streaming of big data. <br>
          - You can control logical access to Amazon Kinesis resources and management functions by creating users under your AWS account using AWS IAM and controlling which Amazon Kinesis operations these users have permission to perform.<br>
          - The Amazon Kinesis API is only accessible via an SSL-encrypted endpoint to help ensure sucure transmission of your data to AWS.
        </li>

        <li>
          <h4>Deployment and Management - AWS IAM</h4>
          - AWS IAM allows you to create mulitiple users and manage the permissions for each of these users within your AWS account. <br>
          - A user is an identity (within AWS account) with unique credentials that can be used to access AWS Cloud service. <br>
          - IAM is secure by default; new users have no access to AWS until permissions are explicitly granted. <br>
          - A role is set of permissions to access specific AWS resources, but these permissions are not tied to particular IAM user or group.
        </li>

        <li>
          <h4>Mobile Services - Amazon Cognito</h4>
          - Amazon Cognito provides identity and sync services for mobile and web-based applications. <br>
          - Your applicaiton authenticates with one of the well-known identity providers such as Google, Facebook, and Amazon using the provider's SDK. <br>
          - After the end use authenticated with the provider, an OAuth or OpenID Connect token returned from provider is passed by your application to Amazon Cognito, whcih returns a new Amazon Congito ID for the user and a set of temporary, limited-privilege AWS credentials.<br>
        </li>

        <li>
          <h4>Applications - Amazon Workspaces</h4>
          - Amazon Workspaces is a managed desktop service that allows you to quickly provision cloud-based desktops for your users. <br>
          - Amazon Workspaces uses PCoIP, which provides an interactive vedio stream without transmitting actual data. <br>
          - The PCoIP protocol compresses, encrypt, and endcodes the user's desktop computing experience and transmits as pixels only across any standard IP network to end-user devices. <br>
          - In order to access their WorkSpace, users must sign in using set of unique credentials and their regular Active Directory credentials. <br>
          - You can also require the use of MFA upon sign-in in the form of a hardware or software token. <br>
          - Amazon Workspaces supports MFA using an on-promise RADIUS server or any security provider that support RADIUS authentication. <br>
          - It currenlty supports that PAP, CHAP, MS-CHAP1, and MS-CHAP2 protocols, along with REDIUS proxies.
        </li>
      </ul>
    </div>


    <div id="chapter13">
      <h3>13. AWS Risk and Compliance</h3>
      <ul>
        <li>
            AWS communicates with customers regarding its security and control envronment through the following mechanism: <br>
            - Obtaining industry certification and independent third-party attenstations. <br>
            - Publishing information about secuirity and AWS control practices via website, whitepapers, and blogs. <br>
            - Directly providing customers with certificates, reports, and other documentation (Under NDA in somecases).
          </li>
          <li>
            - The shared responsibility model is not just limited to security considerations; it also extends IT controls. <br>
            - The management, operation, and verification of IT controls are shared between AWS and the customers. <br>
            - AWS manages these controls where it relates to the physical infrastructure, and the customer manages these controls for the guest operating systems and upward (depending on the service). <br>
            - It is the customer's responsibility to maintain adequet governance over the entire IT control envronment, regardless of how their IT is deployed (on-promises, cloud, or hybrid). <br>
            - By deploying to AWS Cloud, customers have different options for applying different types of controls and various verification methods that align with their business requirements.
          </li>
          <li>
            - The control envronment for AWS contains large volume of information. This information is provided to customers through whitepapers, reports, certications, and other third-party attestations. <br>
            - AWS provides IT control information to customers in two ways : specific control definition and general control standard compiance. <br>
            - AWS provides documentation about its risk and compliance programs. This documentation can enable customers to include AWS controls in their governance framework. <br>
            - The three core areas of risk and compliance program are risk management, control environment, and information security.
          </li>
          <li>
            AWS has achieved a number of internationally organized certifications and accreditations that demonstrate AWS compliance with third-party assurance frameworks, including: <br>
            - FedRAMP <br>
            - FIPS 140-2 <br>
            - FISMA and DIACAP <br>
            - HIPAA <br>
            - ISO 9001 <br>
            - ISO 27001 <br>
            - ITAR <br>
            - PCI DSS Level 1 <br>
            - SOC 1/ ISAE 3402 <br>
            - SOC 2 <br>
            - SOC 3
            AWS is constantly listening to customers and examining other certificaiton for future.
          </li>
          </li>

      </ul>
    </div>


    <div id="chapter14">
      <h3>14. Architecture Best Practices</h3>
      <ul>
        <li>
           - Typically, production systems come with defined or implicit requirements in terms of uptime. <br>
           - A system is highly available when it can withdstand the failure of an individual or multiple components. <br>
           - If you design architecture around the assumption that any component will eventually fail, system won't fail when an individual component does.
         </li>
         <li>
           - Traditional infrastructure generally necessitates predicting the amount of computing resources your application will use over a period of several years. <br>
           - If you underestimate, your applicaitons will not have the horsepower to handle unexpected traffic, potentially resulting in customer dissatisfaction.<br>
           - If you overestimate, you're wasting money with superflous resources.
           - The on-demand and elasitc nature of cloud enables the infrastructure to be closely aligned with actual demand, there by increasing overall utilization and reducing cost. <br>
           - While cloud computing provides vitually unlimited on-demand capacity, system architecture need to be able to take advantage of those resources seamlessly. <br>
           - There are generatlly two ways to scale and IT architecture: vertically and horizantally.
         </li>
         <li>
           - AWS Clould provides governance capabilities that enable contineous monitoring of configuration changes to yout IT resources. <br>
           - Because AWS assets are programmable resources, your security policy can be formalized and embedded with the design of your infrastructure. <br>
           - With the ability to spin up temporary environments, security testing can now be part of your contineous delivery pipeline. <br>
           - Solution Architects can leverage a plethora of native AWS security and encryption features that can help achieve higher levels of data protection and compliance at every layer of cloud infrastructures.
         </li>
         <li>
           - Because AWS makes parallelization effortless, Solution Architects need to internalize the concept of parallelization when designing architectures in the cloud. <br>
           - It is advisable not only implement parallelization wherever possible, but also to automate it, because the cloud allows you to create a repeatable process very easily.
           - As application complexity increases, a disirable characteristics of an IT system is that it can be broken into smaller, loosly coupled components. <br>
           - Soultion Architects should design systems in a way that reduces interdependencies, so that a change or a failure in one componenet does not casecade to other components.
         </li>
         <li>
           - When organization try to map their existing system specifications to those available in the cloud, they notice that cloud might not have exact specification of the resource that they have on-promises. <br>
           - Organizations should not be afraid and feel constrained when using cloud resources. <br>
           - Even if you might not get an exact replica of your hardware in the clould environment, you have the ability to get more of those resources in the cloud to compensate. <br>
           - By focusing on concepts and best practices - like designinng for failures, decoupling the application components, understanding and implementing elasticity, combining it with parallelization, and integrating security in every aspect of the applicaiton architecture - Solution Architect can understand the design considerations necessary for building highly scalable cloud applications.
         </li>
         <li>
           - As each use case is unique, Solution Architects need to remain diligent in evaluating how best practices and patterns can be applied for each implementation. <br>
           - The topic of cloud computing architecture is braod and contineously evolving.
         </li>
      </ul>
    </div>